{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd0ff67",
   "metadata": {},
   "source": [
    "# Notebook 3c - Soil Type Engineering\n",
    "\n",
    "In this notebook, we use soil type features to engineer new features using interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for testing changes to this notebook quickly\n",
    "RANDOM_SEED = 0\n",
    "NUM_FOLDS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pyarrow\n",
    "import gc\n",
    "\n",
    "# Model evaluation\n",
    "from functools import partial\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf981568",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a25690",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load original data\n",
    "original = pd.read_feather('../data/original.feather')\n",
    "\n",
    "# Label Encode\n",
    "old_encoder = LabelEncoder()\n",
    "original[\"Cover_Type\"] = old_encoder.fit_transform(original[\"Cover_Type\"])\n",
    "y_train = original['Cover_Type'].iloc[:15119]\n",
    "y_test = original['Cover_Type'].iloc[15119:]\n",
    "\n",
    "# Get feature columns\n",
    "features = [x for x in original.columns if x not in ['Id','Cover_Type']]\n",
    "\n",
    "# Data structures for summary scores\n",
    "bagging_scores = list()\n",
    "extratrees_scores = list()\n",
    "adaboost_scores = list()\n",
    "random_scores = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a66116f",
   "metadata": {},
   "source": [
    "# Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef3fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_original(sklearn_model, processing = None):\n",
    "    \n",
    "    # Original Training/Test Split\n",
    "    X_temp = original[features].iloc[:15119]\n",
    "    X_test = original[features].iloc[15119:]\n",
    "    y_temp = original['Cover_Type'].iloc[:15119]\n",
    "    y_test = original['Cover_Type'].iloc[15119:]\n",
    "    \n",
    "    # Feature Engineering\n",
    "    if processing:\n",
    "        X_temp = processing(X_temp)\n",
    "        X_test = processing(X_test)\n",
    "        \n",
    "    # Store the out-of-fold predictions\n",
    "    test_preds = np.zeros((X_test.shape[0],7))\n",
    "    oof_preds = np.zeros((X_temp.shape[0],))\n",
    "    scores, times = np.zeros(NUM_FOLDS), np.zeros(NUM_FOLDS)\n",
    "    \n",
    "    # Stratified k-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(X_temp,y_temp)):\n",
    "       \n",
    "        # Training and Validation Sets\n",
    "        X_train, X_valid = X_temp.iloc[train_idx], X_temp.iloc[valid_idx]\n",
    "        y_train, y_valid = y_temp.iloc[train_idx], y_temp.iloc[valid_idx]\n",
    "        \n",
    "        # Create model\n",
    "        start = time.time()\n",
    "        model = clone(sklearn_model)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # validation and test predictions\n",
    "        valid_preds = np.ravel(model.predict(X_valid))\n",
    "        oof_preds[valid_idx] = valid_preds\n",
    "        test_preds += model.predict_proba(X_test)\n",
    "        \n",
    "        # Save scores and times\n",
    "        scores[fold] = accuracy_score(y_valid, valid_preds)\n",
    "        end = time.time()\n",
    "        times[fold] = end-start\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    test_preds = np.argmax(test_preds, axis = 1)\n",
    "    test_score = accuracy_score(y_test, test_preds)\n",
    "    print('\\n'+model.__class__.__name__)\n",
    "    print(\"Train Accuracy:\", round(scores.mean(), 5))\n",
    "    print('Test Accuracy:', round(test_score, 5))\n",
    "    print(f'Training Time: {round(times.sum(), 2)}s')\n",
    "    \n",
    "    return scores.mean(), oof_preds, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78094bd",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "We use the following 4 models from the scikit-learn library:\n",
    "\n",
    "1. AdaBoost \n",
    "2. ExtraTrees\n",
    "3. Bagging\n",
    "4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classifier\n",
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator = DecisionTreeClassifier(\n",
    "        splitter = 'random',\n",
    "        random_state = RANDOM_SEED,\n",
    "    ),\n",
    "    random_state = RANDOM_SEED,\n",
    ")\n",
    "\n",
    "# ExtraTrees Classifier\n",
    "extratrees = ExtraTreesClassifier(\n",
    "    n_jobs = -1,\n",
    "    random_state = RANDOM_SEED,\n",
    "    max_features = None,\n",
    ")\n",
    "\n",
    "# Bagging Classifier\n",
    "bagging = BaggingClassifier(\n",
    "    base_estimator = DecisionTreeClassifier(\n",
    "        splitter = 'random',\n",
    "        random_state = RANDOM_SEED,\n",
    "    ),\n",
    "    n_jobs = -1,\n",
    "    random_state = RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Random Forest Classifier\n",
    "randomforest = RandomForestClassifier(\n",
    "    n_jobs = -1,\n",
    "    random_state = RANDOM_SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30059f",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d433f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "cv_score, oof_preds, test_score = train_original(adaboost)\n",
    "\n",
    "adaboost_scores.append((\n",
    "    'Baseline', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))\n",
    "\n",
    "# ExtraTrees\n",
    "cv_score, oof_preds, test_score = train_original(extratrees)\n",
    "\n",
    "extratrees_scores.append((\n",
    "    'Baseline', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))\n",
    "\n",
    "# Bagging\n",
    "cv_score, oof_preds, test_score = train_original(bagging)\n",
    "\n",
    "bagging_scores.append((\n",
    "    'Baseline', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))\n",
    "\n",
    "cv_score, oof_preds, test_score = train_original(randomforest)\n",
    "\n",
    "random_scores.append((\n",
    "    'Baseline', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a34bef",
   "metadata": {},
   "source": [
    "# Categorial Feature Interactions\n",
    "\n",
    "We test out the following interactions:\n",
    "\n",
    "1. Climatic Zone and Wilderness Area\n",
    "2. Geologic Zone and Wilderness Area\n",
    "3. Surface Cover and Wilderness Area\n",
    "4. Rock Size and Wilderness Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f50912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_soil_types(input_df, drop = True):\n",
    "    data = input_df.copy()\n",
    "    data['Soil_Type'] = 0\n",
    "    soil_features = list()\n",
    "    for i in range(1,41):\n",
    "        data['Soil_Type'] += i*data[f'Soil_Type{i}']\n",
    "        soil_features.append(f'Soil_Type{i}')\n",
    "    if drop:\n",
    "        nonsoil_features = [x for x in data.columns if x not in soil_features]\n",
    "        return data[nonsoil_features]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11a9ce",
   "metadata": {},
   "source": [
    "## 1. Climatic Zone and Wilderness Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bab40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def climatic_zone_original(input_df):\n",
    "    code = {\n",
    "        1:2702,2:2703,3:2704,4:2705,5:2706,6:2717,7:3501,8:3502,9:4201,\n",
    "        10:4703,11:4704,12:4744,13:4758,14:5101,15:5151,16:6101,17:6102,\n",
    "        18:6731,19:7101,20:7102,21:7103,22:7201,23:7202,24:7700,25:7701,\n",
    "        26:7702,27:7709,28:7710,29:7745,30:7746,31:7755,32:7756,33:7757,\n",
    "        34:7790,35:8703,36:8707,37:8708,38:8771,39:8772,40:8776\n",
    "    }\n",
    "    data = consolidate_soil_types(input_df, drop = False)\n",
    "    df = input_df.copy()\n",
    "    df['Climatic_Zone'] = data['Soil_Type'].apply(lambda x: int(str(code[x])[0]))\n",
    "    return df\n",
    "\n",
    "def wilderness_climatic(input_df, drop = False):\n",
    "    data = climatic_zone_original(input_df)\n",
    "    df = input_df.copy()\n",
    "    df['Climate_Area1'] = df['Wilderness_Area1']*data['Climatic_Zone'] \n",
    "    df['Climate_Area2'] = df['Wilderness_Area2']*data['Climatic_Zone'] \n",
    "    df['Climate_Area3'] = df['Wilderness_Area3']*data['Climatic_Zone'] \n",
    "    df['Climate_Area4'] = df['Wilderness_Area4']*data['Climatic_Zone'] \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "cv_score, oof_preds, test_score = train_original(adaboost, wilderness_climatic)\n",
    "\n",
    "adaboost_scores.append((\n",
    "    'Wild_Clim', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))\n",
    "\n",
    "# ExtraTrees\n",
    "cv_score, oof_preds, test_score = train_original(extratrees, wilderness_climatic)\n",
    "\n",
    "extratrees_scores.append((\n",
    "    'Wild_Clim', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))\n",
    "\n",
    "# Bagging\n",
    "cv_score, oof_preds, test_score = train_original(bagging, wilderness_climatic)\n",
    "\n",
    "bagging_scores.append((\n",
    "    'Wild_Clim', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))\n",
    "\n",
    "# RandomForest\n",
    "cv_score, oof_preds, test_score = train_original(randomforest, wilderness_climatic)\n",
    "\n",
    "random_scores.append((\n",
    "    'Wild_Clim', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffabc2a",
   "metadata": {},
   "source": [
    "## 2. Geologic Zone and Wilderness Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ff6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geologic_zone_original(input_df):\n",
    "    code = {\n",
    "        1:2702,2:2703,3:2704,4:2705,5:2706,6:2717,7:3501,8:3502,9:4201,\n",
    "        10:4703,11:4704,12:4744,13:4758,14:5101,15:5151,16:6101,17:6102,\n",
    "        18:6731,19:7101,20:7102,21:7103,22:7201,23:7202,24:7700,25:7701,\n",
    "        26:7702,27:7709,28:7710,29:7745,30:7746,31:7755,32:7756,33:7757,\n",
    "        34:7790,35:8703,36:8707,37:8708,38:8771,39:8772,40:8776\n",
    "    }\n",
    "    data = consolidate_soil_types(input_df, drop = False)\n",
    "    df = input_df.copy()\n",
    "    df['Geologic_Zone'] = data['Soil_Type'].apply(lambda x: int(str(code[x])[1]))\n",
    "    return df\n",
    "\n",
    "def wilderness_geologic(input_df, drop = False):\n",
    "    data = geologic_zone_original(input_df)\n",
    "    df = input_df.copy()\n",
    "    df['Geologic_Area1'] = df['Wilderness_Area1']*data['Geologic_Zone'] \n",
    "    df['Geologic_Area2'] = df['Wilderness_Area2']*data['Geologic_Zone']  \n",
    "    df['Geologic_Area3'] = df['Wilderness_Area3']*data['Geologic_Zone'] \n",
    "    df['Geologic_Area4'] = df['Wilderness_Area4']*data['Geologic_Zone'] \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3184d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "cv_score, oof_preds, test_score = train_original(adaboost, wilderness_geologic)\n",
    "\n",
    "adaboost_scores.append((\n",
    "    'Wild_Geo', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))\n",
    "\n",
    "# ExtraTrees\n",
    "cv_score, oof_preds, test_score = train_original(extratrees, wilderness_geologic)\n",
    "\n",
    "extratrees_scores.append((\n",
    "    'Wild_Geo', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))\n",
    "\n",
    "# Bagging\n",
    "cv_score, oof_preds, test_score = train_original(bagging, wilderness_geologic)\n",
    "\n",
    "bagging_scores.append((\n",
    "    'Wild_Geo', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))\n",
    "\n",
    "# RandomForest\n",
    "cv_score, oof_preds, test_score = train_original(randomforest, wilderness_geologic)\n",
    "\n",
    "random_scores.append((\n",
    "    'Wild_Geo', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496275f",
   "metadata": {},
   "source": [
    "## 3. Surface Cover and Wilderness Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e37247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surface_cover_original(input_df):\n",
    "    # Group IDs\n",
    "    no_desc = [7,8,14,15,16,17,19,20,21,23,35]\n",
    "    stony = [6,12]\n",
    "    very_stony = [2,9,18,26]\n",
    "    extremely_stony = [1,22,24,25,27,28,29,30,31,32,33,34,36,37,38,39,40]\n",
    "    rubbly = [3,4,5,10,11,13]\n",
    "\n",
    "    # Create dictionary\n",
    "    surface_cover = {i:0 for i in no_desc}\n",
    "    surface_cover.update({i:1 for i in stony})\n",
    "    surface_cover.update({i:2 for i in very_stony})\n",
    "    surface_cover.update({i:3 for i in extremely_stony})\n",
    "    surface_cover.update({i:4 for i in rubbly})\n",
    "    \n",
    "    # Create Feature\n",
    "    data = consolidate_soil_types(input_df, drop = False)\n",
    "    df = input_df.copy()\n",
    "    df['Surface_Cover'] = data['Soil_Type'].apply(lambda x: surface_cover[x])\n",
    "    return df\n",
    "\n",
    "def wilderness_surface(input_df, drop = False):\n",
    "    data = surface_cover_original(input_df)\n",
    "    df = input_df.copy()\n",
    "    df['Surface_Area1'] = df['Wilderness_Area1']*data['Surface_Cover'] \n",
    "    df['Surface_Area2'] = df['Wilderness_Area2']*data['Surface_Cover']   \n",
    "    df['Surface_Area3'] = df['Wilderness_Area3']*data['Surface_Cover']  \n",
    "    df['Surface_Area4'] = df['Wilderness_Area4']*data['Surface_Cover'] \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07cd25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "cv_score, oof_preds, test_score = train_original(adaboost, wilderness_surface)\n",
    "\n",
    "adaboost_scores.append((\n",
    "    'Wild_Surf', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))\n",
    "\n",
    "# ExtraTrees\n",
    "cv_score, oof_preds, test_score = train_original(extratrees, wilderness_surface)\n",
    "\n",
    "extratrees_scores.append((\n",
    "    'Wild_Surf', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))\n",
    "\n",
    "# Bagging\n",
    "cv_score, oof_preds, test_score = train_original(bagging, wilderness_surface)\n",
    "\n",
    "bagging_scores.append((\n",
    "    'Wild_Surf', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))\n",
    "\n",
    "# RandomForest\n",
    "cv_score, oof_preds, test_score = train_original(randomforest, wilderness_surface)\n",
    "\n",
    "random_scores.append((\n",
    "    'Wild_Surf', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8be1e18",
   "metadata": {},
   "source": [
    "## 4. Rock Size and Wilderness Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rock_size_original(input_df):\n",
    "    \n",
    "    # Group IDs\n",
    "    no_desc = [7,8,14,15,16,17,19,20,21,23,35]\n",
    "    stones = [1,2,6,9,12,18,24,25,26,27,28,29,30,31,32,33,34,36,37,38,39,40]\n",
    "    boulders = [22]\n",
    "    rubble = [3,4,5,10,11,13]\n",
    "\n",
    "    # Create dictionary\n",
    "    rock_size = {i:0 for i in no_desc}\n",
    "    rock_size.update({i:1 for i in stones})\n",
    "    rock_size.update({i:2 for i in boulders})\n",
    "    rock_size.update({i:3 for i in rubble})\n",
    "    \n",
    "    data = consolidate_soil_types(input_df, drop = False)\n",
    "    df = input_df.copy()\n",
    "    df['Rock_Size'] = data['Soil_Type'].apply(lambda x: rock_size[x])\n",
    "    return df\n",
    "\n",
    "def wilderness_rocksize(input_df, drop = False):\n",
    "    data = rock_size_original(input_df)\n",
    "    df = input_df.copy()\n",
    "    df['Rock_Area1'] = df['Wilderness_Area1']*data['Rock_Size'] \n",
    "    df['Rock_Area2'] = df['Wilderness_Area2']*data['Rock_Size']   \n",
    "    df['Rock_Area3'] = df['Wilderness_Area3']*data['Rock_Size']  \n",
    "    df['Rock_Area4'] = df['Wilderness_Area4']*data['Rock_Size']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feedc1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "cv_score, oof_preds, test_score = train_original(adaboost, wilderness_rocksize)\n",
    "\n",
    "adaboost_scores.append((\n",
    "    'Wild_Rock', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))\n",
    "\n",
    "# ExtraTrees\n",
    "cv_score, oof_preds, test_score = train_original(extratrees, wilderness_rocksize)\n",
    "\n",
    "extratrees_scores.append((\n",
    "    'Wild_Rock', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))\n",
    "\n",
    "# Bagging\n",
    "cv_score, oof_preds, test_score = train_original(bagging, wilderness_rocksize)\n",
    "\n",
    "bagging_scores.append((\n",
    "    'Wild_Rock', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))\n",
    "\n",
    "# RandomForest\n",
    "cv_score, oof_preds, test_score = train_original(randomforest, wilderness_rocksize)\n",
    "\n",
    "random_scores.append((\n",
    "    'Wild_Rock', cv_score, test_score,\n",
    "     *recall_score(y_train, oof_preds, average = None)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f71fe",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "pd.DataFrame.from_records(\n",
    "    data = adaboost_scores,\n",
    "    columns = ['features','cv_score','holdout','recall_0', 'recall_1','recall_2','recall_3','recall_4','recall_5','recall_6']\n",
    ").sort_values('holdout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees Classifier\n",
    "pd.DataFrame.from_records(\n",
    "    data = extratrees_scores,\n",
    "    columns = ['features','cv_score','holdout','recall_0', 'recall_1','recall_2','recall_3','recall_4','recall_5','recall_6']\n",
    ").sort_values('holdout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c1a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging Classifier\n",
    "pd.DataFrame.from_records(\n",
    "    data = bagging_scores,\n",
    "    columns = ['features','cv_score','holdout','recall_0', 'recall_1','recall_2','recall_3','recall_4','recall_5','recall_6']\n",
    ").sort_values('holdout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "pd.DataFrame.from_records(\n",
    "    data = random_scores,\n",
    "    columns = ['features','cv_score','holdout','recall_0', 'recall_1','recall_2','recall_3','recall_4','recall_5','recall_6']\n",
    ").sort_values('holdout')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
